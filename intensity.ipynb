{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PoP Intensity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a deeper dive into the project and to explore all the details, please visit our [README file](readme.md). You'll find everything you need to know, from the project's background and objectives to the nitty-gritty of how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Date: 2025.01.05\n",
    "- Researchers:\n",
    "    - Dr. Lázaro Bustio-Martínez\n",
    "    - Dr. Vitali Herrera-Semenets\n",
    "    - Dr. Miguel Á. Álvarez-Carmona\n",
    "    - Dr. Jorge Á. González-Ordiano\n",
    "    - Dr. Jan van den Berg\n",
    "    - Dr. Pedro Á. Santander-Molina\n",
    "- Contact: [Dr. Lázaro Bustio Martínez](lazaro.bustio@ibero.mx).\n",
    "- Python version: 3.12.8\n",
    "\n",
    "Determine the intensity level for each principle of persuasion. This code is developed for Python 3.12.8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-05 21:34:38: \u001b[1;32mINFO\u001b[0m - \u001b[0;32mThe code has started running correctly.\u001b[0m\n",
      "2025-01-05 21:34:38: \u001b[1;33mWARNING\u001b[0m - \u001b[0;33mThis is a warning message.\u001b[0m\n",
      "2025-01-05 21:34:38: \u001b[1;31mERROR\u001b[0m - \u001b[0;31mAn error occurred in the code.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "# Colores ANSI\n",
    "class LogColors:\n",
    "    INFO_BOLD = \"\\033[1;32m\"  # Verde intenso (negrita) para INFO\n",
    "    INFO = \"\\033[0;32m\"  # Verde suave para INFO (esto lo cambiaré a verde lima)\n",
    "    WARNING_BOLD = \"\\033[1;33m\"  # Amarillo intenso (negrita) para WARNING\n",
    "    WARNING = \"\\033[0;33m\"  # Amarillo suave para WARNING\n",
    "    ERROR_BOLD = \"\\033[1;31m\"  # Rojo intenso (negrita) para ERROR\n",
    "    ERROR = \"\\033[0;31m\"  # Rojo suave para ERROR\n",
    "    RESET = \"\\033[0m\"  # Resetear el color\n",
    "\n",
    "# Formateador personalizado\n",
    "class ColoredFormatter(logging.Formatter):\n",
    "    def format(self, record):\n",
    "        # Colorear el tipo de mensaje (levelname) y el mensaje del log\n",
    "        if record.levelname == \"INFO\":\n",
    "            record.levelname = f\"{LogColors.INFO_BOLD}{record.levelname}{LogColors.RESET}\"\n",
    "            record.msg = f\"{LogColors.INFO}{record.msg}{LogColors.RESET}\"\n",
    "        elif record.levelname == \"WARNING\":\n",
    "            record.levelname = f\"{LogColors.WARNING_BOLD}{record.levelname}{LogColors.RESET}\"\n",
    "            record.msg = f\"{LogColors.WARNING}{record.msg}{LogColors.RESET}\"\n",
    "        elif record.levelname == \"ERROR\":\n",
    "            record.levelname = f\"{LogColors.ERROR_BOLD}{record.levelname}{LogColors.RESET}\"\n",
    "            record.msg = f\"{LogColors.ERROR}{record.msg}{LogColors.RESET}\"\n",
    "       \n",
    "        return super().format(record)\n",
    "\n",
    "# Configuración del logger\n",
    "logger = logging.getLogger(\"MiLogger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Evitar manejadores duplicados\n",
    "logger.handlers.clear()\n",
    "\n",
    "# Manejador de consola con colores\n",
    "console_handler = logging.StreamHandler()\n",
    "formato = ColoredFormatter(\"%(asctime)s: %(levelname)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
    "console_handler.setFormatter(formato)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "# Desactivar el logger raíz (limpiar los manejadores raíz)\n",
    "logging.getLogger().handlers.clear()\n",
    "\n",
    "# Ejemplo de uso\n",
    "logger.info(\"The code has started running correctly.\")\n",
    "logger.warning(\"This is a warning message.\")\n",
    "logger.error(\"An error occurred in the code.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid compatibility issues among different environments, all the necessary libraries are first installed and then imported. Any additional libraries needed should be installed in this cell before importing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Starting the installation of required libraries...\")\n",
    "\n",
    "%pip install pandas\n",
    "%pip install numpy\n",
    "%pip install matplotlib\n",
    "%pip install seaborn\n",
    "%pip install scipy\n",
    "%pip install scikit-learn\n",
    "%pip install transformers\n",
    "%pip install prettytable\n",
    "%pip install torch\n",
    "%pip install psutil \n",
    "%pip install gputil\n",
    "%pip install pandasgui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After installing the libraries, then they can be imported and used in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-05 21:34:47: \u001b[1;32mINFO\u001b[0m - \u001b[0;32mStarting the import of necessary libraries for the project...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Starting the import of necessary libraries for the project...\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import scipy.stats as stats\n",
    "from scipy.spatial.distance import cdist\n",
    "import sklearn\n",
    "from sklearn.cluster import KMeans, DBSCAN, OPTICS, AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score, \n",
    "    silhouette_samples, \n",
    "    pairwise_distances, \n",
    "    euclidean_distances\n",
    ")\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import prettytable\n",
    "from prettytable import PrettyTable\n",
    "import torch\n",
    "import platform\n",
    "import psutil\n",
    "import GPUtil\n",
    "from transformers import __version__ as transformers_version\n",
    "import json\n",
    "import os\n",
    "from pandasgui import show\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, to avoid compatibility issues, generate the requirements file from the libraries used ans save it in `requirements.txt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-05 21:34:58: \u001b[1;32mINFO\u001b[0m - \u001b[0;32m'requirements.txt' file has been generated successfully!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Generar requirements.txt \n",
    "%pip freeze > requirements.txt\n",
    "logger.info(\"'requirements.txt' file has been generated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the versions of the utilized libraries to ensure compatibility and consistency across different environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-05 21:35:03: \u001b[1;32mINFO\u001b[0m - \u001b[0;32mLibrary Versions:\u001b[0m\n",
      "2025-01-05 21:35:03: \u001b[1;32mINFO\u001b[0m - \u001b[0;32mpandas version: 2.2.3\u001b[0m\n",
      "2025-01-05 21:35:03: \u001b[1;32mINFO\u001b[0m - \u001b[0;32mnumpy version: 2.2.1\u001b[0m\n",
      "2025-01-05 21:35:03: \u001b[1;32mINFO\u001b[0m - \u001b[0;32mmatplotlib version: 3.10.0\u001b[0m\n",
      "2025-01-05 21:35:03: \u001b[1;32mINFO\u001b[0m - \u001b[0;32mscipy version: 1.15.0\u001b[0m\n",
      "2025-01-05 21:35:03: \u001b[1;32mINFO\u001b[0m - \u001b[0;32mscikit-learn version: 1.6.0\u001b[0m\n",
      "2025-01-05 21:35:03: \u001b[1;32mINFO\u001b[0m - \u001b[0;32mtransformers version: 4.47.1\u001b[0m\n",
      "2025-01-05 21:35:03: \u001b[1;32mINFO\u001b[0m - \u001b[0;32mprettytable version: 3.12.0\u001b[0m\n",
      "2025-01-05 21:35:03: \u001b[1;32mINFO\u001b[0m - \u001b[0;32mtorch version: 2.5.1+cpu\u001b[0m\n",
      "2025-01-05 21:35:03: \u001b[1;32mINFO\u001b[0m - \u001b[0;32mSystem details:\u001b[0m\n",
      "2025-01-05 21:35:03: \u001b[1;32mINFO\u001b[0m - \u001b[0;32mPython version: 3.12.8\u001b[0m\n",
      "2025-01-05 21:35:03: \u001b[1;32mINFO\u001b[0m - \u001b[0;32mPlatform: Windows 11\u001b[0m\n",
      "2025-01-05 21:35:03: \u001b[1;32mINFO\u001b[0m - \u001b[0;32mProcessor: Intel64 Family 6 Model 186 Stepping 2, GenuineIntel\u001b[0m\n",
      "2025-01-05 21:35:03: \u001b[1;32mINFO\u001b[0m - \u001b[0;32mArchitecture: 64bit\u001b[0m\n",
      "2025-01-05 21:35:03: \u001b[1;32mINFO\u001b[0m - \u001b[0;32mNumber of CPUs: 16\u001b[0m\n",
      "2025-01-05 21:35:03: \u001b[1;32mINFO\u001b[0m - \u001b[0;32mTotal RAM: 15.73 GB\u001b[0m\n",
      "2025-01-05 21:35:03: \u001b[1;33mWARNING\u001b[0m - \u001b[0;33mNo GPU detected\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def get_library_versions():\n",
    "    logger.info(f\"pandas version: {pd.__version__}\")\n",
    "    logger.info(f\"numpy version: {np.__version__}\")\n",
    "    logger.info(f\"matplotlib version: {matplotlib.__version__}\")\n",
    "    logger.info(f\"scipy version: {scipy.__version__}\")\n",
    "    logger.info(f\"scikit-learn version: {sklearn.__version__}\")\n",
    "    logger.info(f\"transformers version: {transformers_version}\")\n",
    "    logger.info(f\"prettytable version: {prettytable.__version__}\")\n",
    "    logger.info(f\"torch version: {torch.__version__}\")\n",
    "\n",
    "def get_system_info():\n",
    "    logger.info(f\"Python version: {platform.python_version()}\")\n",
    "    logger.info(f\"Platform: {platform.system()} {platform.release()}\")\n",
    "    logger.info(f\"Processor: {platform.processor()}\")\n",
    "    logger.info(f\"Architecture: {platform.architecture()[0]}\")\n",
    "    logger.info(f\"Number of CPUs: {psutil.cpu_count(logical=True)}\")\n",
    "    logger.info(f\"Total RAM: {psutil.virtual_memory().total / (1024 ** 3):.2f} GB\")\n",
    "    \n",
    "    gpus = GPUtil.getGPUs()\n",
    "    if gpus:\n",
    "        for gpu in gpus:\n",
    "            logger.info(f\"GPU: {gpu.name}\")\n",
    "            logger.info(f\"  GPU Memory Total: {gpu.memoryTotal} MB\")\n",
    "            logger.info(f\"  GPU Memory Free: {gpu.memoryFree} MB\")\n",
    "            logger.info(f\"  GPU Memory Used: {gpu.memoryUsed} MB\")\n",
    "            logger.info(f\"  GPU Utilization: {gpu.memoryUtil * 100}%\")\n",
    "    else:\n",
    "        logger.warning(\"No GPU detected\")\n",
    "\n",
    "logger.info(\"Library Versions:\")\n",
    "get_library_versions()\n",
    "\n",
    "logger.info(\"System details:\")\n",
    "get_system_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data gathering and filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phishing messages and principles of persuasion are stored in `data\\pop_dataset_Full(Tiltan).csv` that needs to be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-05 21:35:13: \u001b[1;32mINFO\u001b[0m - \u001b[0;32mReading the data file to begin processing...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>path</th>\n",
       "      <th>hash</th>\n",
       "      <th>subject</th>\n",
       "      <th>txt</th>\n",
       "      <th>authority</th>\n",
       "      <th>class</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1087</th>\n",
       "      <td>9762</td>\n",
       "      <td>/content/gdrive/MyDrive/Colabs/02.-Proyectos/P...</td>\n",
       "      <td>52f8f694517011d944083f22ace9448e</td>\n",
       "      <td>Urgent Fraud Prevention Group Notice</td>\n",
       "      <td>Urgent Fraud Prevention Group Notice You have...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Phish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1088</th>\n",
       "      <td>9763</td>\n",
       "      <td>/content/gdrive/MyDrive/Colabs/02.-Proyectos/P...</td>\n",
       "      <td>b4dab37a7c9be664f7bbd4a5f48e9409</td>\n",
       "      <td>your paypal account user domain</td>\n",
       "      <td>Dear PayPal Inc. account holder PayPal is cons...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Phish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089</th>\n",
       "      <td>9764</td>\n",
       "      <td>/content/gdrive/MyDrive/Colabs/02.-Proyectos/P...</td>\n",
       "      <td>c5a00795f1af36457fa3e6da1ce902ce</td>\n",
       "      <td>Notification Update Your eBay Profile</td>\n",
       "      <td>Dear eBay Customer As the domain.com and infor...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Phish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>9765</td>\n",
       "      <td>/content/gdrive/MyDrive/Colabs/02.-Proyectos/P...</td>\n",
       "      <td>5d668953b7cf30441baffbb296702768</td>\n",
       "      <td>Thanks again for using online payments</td>\n",
       "      <td>E mail Security Information. Dear Customer Tha...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Phish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>9766</td>\n",
       "      <td>/content/gdrive/MyDrive/Colabs/02.-Proyectos/P...</td>\n",
       "      <td>0009311d8938d26b6807618aa3febc78</td>\n",
       "      <td>PayPal Flagged Account</td>\n",
       "      <td>Dear PayPal Member Your account has been rando...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Phish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               path  \\\n",
       "1087  9762  /content/gdrive/MyDrive/Colabs/02.-Proyectos/P...   \n",
       "1088  9763  /content/gdrive/MyDrive/Colabs/02.-Proyectos/P...   \n",
       "1089  9764  /content/gdrive/MyDrive/Colabs/02.-Proyectos/P...   \n",
       "1090  9765  /content/gdrive/MyDrive/Colabs/02.-Proyectos/P...   \n",
       "1091  9766  /content/gdrive/MyDrive/Colabs/02.-Proyectos/P...   \n",
       "\n",
       "                                  hash  \\\n",
       "1087  52f8f694517011d944083f22ace9448e   \n",
       "1088  b4dab37a7c9be664f7bbd4a5f48e9409   \n",
       "1089  c5a00795f1af36457fa3e6da1ce902ce   \n",
       "1090  5d668953b7cf30441baffbb296702768   \n",
       "1091  0009311d8938d26b6807618aa3febc78   \n",
       "\n",
       "                                      subject  \\\n",
       "1087    Urgent Fraud Prevention Group Notice    \n",
       "1088          your paypal account user domain   \n",
       "1089    Notification Update Your eBay Profile   \n",
       "1090  Thanks again for using online payments    \n",
       "1091                   PayPal Flagged Account   \n",
       "\n",
       "                                                    txt  authority  class  \\\n",
       "1087   Urgent Fraud Prevention Group Notice You have...          1      1   \n",
       "1088  Dear PayPal Inc. account holder PayPal is cons...          1      1   \n",
       "1089  Dear eBay Customer As the domain.com and infor...          1      1   \n",
       "1090  E mail Security Information. Dear Customer Tha...          0      1   \n",
       "1091  Dear PayPal Member Your account has been rando...          1      1   \n",
       "\n",
       "      label  \n",
       "1087  Phish  \n",
       "1088  Phish  \n",
       "1089  Phish  \n",
       "1090  Phish  \n",
       "1091  Phish  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger.info(\"Reading the data file to begin processing...\")\n",
    "\n",
    "data_df = pd.read_csv('data/pop_dataset_Full(Tiltan).csv', header=1)\n",
    "pops = [\"authority\"] #[\"authority\", \"distraction\", \"liking_similarity_deception\",\"social_proof\", \"commitment_integrity_reciprocation\"]\n",
    "data_phish = data_df[data_df[\"class\"] == 1]\n",
    "data = data_phish[[\"id\", \"path\", \"hash\", \"subject\", \"txt\"] + pops + [\"class\", \"label\"]]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Messages are stored as text, but they need to be represented in a way that facilitates processing. The data representation method that works best for phishing detection is BERT. BERT (Bidirectional Encoder Representations from Transformers) has the ability to understand the context of words in both directions, enabling better interpretation and analysis of the text. Additionally, BERT has proven highly effective in text classification tasks and detecting anomalous patterns—essential characteristics for identifying phishing messages. Leveraging these capabilities, BERT can more accurately detect malicious messages and protect users against phishing attacks.\n",
    "\n",
    "To implement this, DistilBERT is employed. DistilBERT uses a tokenizer that converts the input text into tokens that the model can process. This model is a smaller, faster, and lighter version of BERT, designed to deliver high performance while requiring fewer computational resources. An informational log entry is added to indicate that both the DistilBERT tokenizer and model have been loaded successfully. By utilizing DistilBERT, which retains 97% of BERT's language understanding capabilities while being more efficient, the system enhances its ability to detect phishing attempts in a streamlined and resource-effective manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-05 21:35:25: \u001b[1;32mINFO\u001b[0m - \u001b[0;32mDistilBERT tokenizer and model have been loaded successfully!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Cargar el tokenizer y el modelo base de DistilBERT\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")  # Modelo base\n",
    "logger.info(\"DistilBERT tokenizer and model have been loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is needed to define a function that processes a list of texts to generate their embeddings using a pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para obtener embeddings del texto\n",
    "def get_embeddings(texts):\n",
    "    logger.info(\"Starting to process texts for embeddings...\")\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    logger.info(f\"Tokenized {len(texts)} texts.\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logger.info(\"Generating embeddings...\")\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Usamos la media de los embeddings de las palabras (output.last_hidden_state)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "    \n",
    "    logger.info(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following it is needed to filter messages according to different principles of persuasion and to calculate their vector representations, known as **embeddings**. To do so, a dictionary `embeddings` is needed, which contains the embeddings generated from the filtered texts, organized by principle of persuasion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-05 21:36:40: \u001b[1;32mINFO\u001b[0m - \u001b[0;32mStarting to filter data for each principle of persuasion\u001b[0m\n",
      "2025-01-05 21:36:40: \u001b[1;32mINFO\u001b[0m - \u001b[0;32mFiltering data for principle: 'authority'...\u001b[0m\n",
      "2025-01-05 21:36:40: \u001b[1;32mINFO\u001b[0m - \u001b[0;32mStarting to process texts for embeddings...\u001b[0m\n",
      "2025-01-05 21:36:41: \u001b[1;32mINFO\u001b[0m - \u001b[0;32mTokenized 598 texts.\u001b[0m\n",
      "2025-01-05 21:36:41: \u001b[1;32mINFO\u001b[0m - \u001b[0;32mGenerating embeddings...\u001b[0m\n",
      "2025-01-05 21:38:20: \u001b[1;32mINFO\u001b[0m - \u001b[0;32mGenerated embeddings with shape: (598, 768)\u001b[0m\n",
      "2025-01-05 21:38:20: \u001b[1;32mINFO\u001b[0m - \u001b[0;32mData filtering completed for all principles.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary of Instances per Principle\n",
      "+-----------+---------------------+\n",
      "| Principle | Number of Instances |\n",
      "+-----------+---------------------+\n",
      "| authority |         598         |\n",
      "+-----------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "# Crear un diccionario para almacenar los DataFrames filtrados\n",
    "embeddings = {} # Contiene los embeddings de los textos filtrados por cada principio de persuasion\n",
    "\n",
    "logger.info(\"Starting to filter data for each principle of persuasion\")\n",
    "\n",
    "for principle in pops:\n",
    "    # Informar sobre el principio actual\n",
    "    logger.info(f\"Filtering data for principle: '{principle}'...\")\n",
    "    \n",
    "    # Filtrar las filas donde el principio en cuestión tenga valor 1\n",
    "    filtered_df = data.loc[data[principle] == 1, ['id', 'path', 'hash', 'subject', 'txt', principle, 'class', 'label']]\n",
    "\n",
    "    # Obtener embeddings para los textos\n",
    "    embed = get_embeddings(filtered_df[\"txt\"].tolist())\n",
    "    \n",
    "    # Convertir las embeddings en DataFrame y agregar las columnas \"label\" y \"class\"\n",
    "    embed_df = pd.DataFrame(embed)\n",
    "    embed_df[\"label\"] = filtered_df[\"label\"].values\n",
    "    embed_df[\"class\"] = filtered_df[\"class\"].values\n",
    "    \n",
    "    # Guardar los embeddings en el diccionario\n",
    "    embeddings[principle] = embed_df\n",
    "\n",
    "logger.info(\"Data filtering completed for all principles.\")\n",
    "\n",
    "# Crear una tabla resumen\n",
    "summary_table = PrettyTable()\n",
    "summary_table.field_names = [\"Principle\", \"Number of Instances\"]\n",
    "\n",
    "# Agregar datos a la tabla resumen\n",
    "for principle, filtered_df in embeddings.items():\n",
    "    summary_table.add_row([principle, filtered_df.shape[0]])\n",
    "\n",
    "# Mostrar la tabla resumen\n",
    "print(\"\\nSummary of Instances per Principle\")\n",
    "print(summary_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the dataset has been processed and embeddings for each principle of persuasion have been generated, the resulting `embeddings` dictionary is saved as a JSON file. This ensures that the data is stored in a compatible and easily accessible format for future use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-05 21:39:32: \u001b[1;32mINFO\u001b[0m - \u001b[0;32mEmbeddings dictionary has been saved to 'results/embeddings_dict.json' sucessfully!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Convertir cada DataFrame en el diccionario embeddings a un formato compatible con JSON\n",
    "embeddings_json = {principle: embed_df.to_dict(orient='records') for principle, embed_df in embeddings.items()}\n",
    "\n",
    "# Guardar el diccionario embeddings en un archivo JSON\n",
    "path = \"results/embeddings_dict.json\"\n",
    "with open(path, 'w') as f:\n",
    "    json.dump(embeddings_json, f, indent=4)\n",
    "\n",
    "logger.info(f\"Embeddings dictionary has been saved to '{path}' sucessfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The premise guiding this research is that messages of equal intensity concerning a specific principle of persuasion should be grouped together. Therefore, it is essential to evaluate various clustering algorithms to identify which one generates the most consistent groups. Additionally, the research aims to verify whether the resulting groups indeed contain messages with similar intensities for each principle of persuasion. This approach not only seeks to enhance the understanding of message intensity but also to ensure that the selected algorithms effectively reflect the underlying similarities among the messages.\n",
    "\n",
    "To achieve this, a pipeline will be implemented in which various clustering algorithms will be evaluated, including k-Means, DBSCAN, OPTICS, and Gaussian Mixture Models (while remaining open to the use of others). The algorithms that require specifying the number of groups to be constructed (such as k-Means and GMM) will analytically determine the optimal number of groups to obtain. The quality of each group will be assessed, and the messages within each group will be analyzed to verify the hypothesis that messages with similar intensities according to a given principle of persuasion will be clustered together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two arrays will be defined to store the clustering algorithms (`clustering_algos`) and their respective configurations (`param_mapping`) that will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_algos = {\n",
    "    'KMeans': KMeans, \n",
    "    'DBSCAN': DBSCAN,\n",
    "    'GaussianMixture': GaussianMixture,\n",
    "    'OPTICS': OPTICS,\n",
    "    'HCA': AgglomerativeClustering\n",
    "}\n",
    "\n",
    "# Diccionario con los parámetros correctos para cada algoritmo\n",
    "param_mapping = {\n",
    "    'KMeans': 'n_clusters',\n",
    "    'GaussianMixture': 'n_components',  # GaussianMixture usa 'n_components'\n",
    "    'DBSCAN': None,  # DBSCAN no usa un parámetro 'n_clusters'\n",
    "    'OPTICS': None,  # OPTICS no usa 'n_clusters'\n",
    "    'HCA': 'n_clusters'  # HCA usa 'n_clusters'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the number of groups emerging from the data is not known a priori (although it can be empirically estimated), it is advisable to automatically determine the optimal number of groups. For this purpose, the [elbow method](https://www.geeksforgeeks.org/elbow-method-for-optimal-value-of-k-in-kmeans/) can be used, which involves exhaustively evaluating different numbers of groups, from 2 to a maximum limit. However, this approach can be computationally expensive. Once the optimal number of groups has been identified, clustering is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_k_elbow_method(X, algorithm_class, principle=None, export_images=False, max_k=10):\n",
    "    inertia = []  # Lista para almacenar la métrica de calidad\n",
    "    K = range(2, max_k + 1, 1)  # Rango de valores para k\n",
    "\n",
    "    for k in K:\n",
    "        if algorithm_class in [DBSCAN, OPTICS]:  \n",
    "            # Para DBSCAN y OPTICS, que no requieren 'n_clusters'\n",
    "            cluster_algo = algorithm_class().fit(X)\n",
    "            \n",
    "            if hasattr(cluster_algo, 'labels_'):\n",
    "                labels = cluster_algo.labels_\n",
    "                unique_labels = np.unique(labels[labels != -1])  # Excluir ruido (-1)\n",
    "                \n",
    "                if len(unique_labels) > 1:\n",
    "                    # Métrica 1: Puntuación de silueta\n",
    "                    silhouette = silhouette_score(X, labels)\n",
    "                    inertia.append(silhouette)\n",
    "                else:\n",
    "                    inertia.append(-1)  # Indicador de resultado no válido\n",
    "            else:\n",
    "                inertia.append(-1)\n",
    "        \n",
    "        elif algorithm_class == GaussianMixture:\n",
    "            # Para Gaussian Mixture\n",
    "            cluster_algo = algorithm_class(n_components=k).fit(X)\n",
    "            labels = cluster_algo.predict(X)\n",
    "            \n",
    "            # Métrica: Puntuación de silueta\n",
    "            if len(np.unique(labels)) > 1:\n",
    "                inertia.append(silhouette_score(X, labels))\n",
    "            else:\n",
    "                inertia.append(-1)\n",
    "        \n",
    "        else:\n",
    "            # Para algoritmos que soportan n_clusters como parámetro (e.g., KMeans, AgglomerativeClustering)\n",
    "            cluster_algo = algorithm_class(n_clusters=k).fit(X)\n",
    "            \n",
    "            if hasattr(cluster_algo, 'inertia_')\n",
    "                # Si el algoritmo tiene el atributo 'inertia_' (e.g., KMeans)\n",
    "                inertia.append(cluster_algo.inertia_)\n",
    "            elif hasattr(cluster_algo, 'labels_') and hasattr(cluster_algo, 'cluster_centers_'):\n",
    "                # Si el algoritmo tiene los atributos 'labels_' y 'cluster_centers_' (e.g., AgglomerativeClustering),\n",
    "                # calcular la inercia manualmente\n",
    "                labels = cluster_algo.labels_\n",
    "                cluster_centers = cluster_algo.cluster_centers_\n",
    "                inertia.append(np.sum(np.min(cdist(X, cluster_centers, 'euclidean'), axis=1)) / X.shape[0])\n",
    "            else:\n",
    "                inertia.append(-1)\n",
    "    \n",
    "    # Calcular el punto del codo aplicando la segunda derivada para encontrar el punto de inflexión\n",
    "    diff = np.diff(inertia)\n",
    "    diff2 = np.diff(diff)\n",
    "    elbow_index = np.argmax(diff2) + 2  + 1 # +2 para ajustar el índice al rango original de K, +1 para el índice 0\n",
    "    optimal_k = elbow_index\n",
    "\n",
    "    # Exportar la gráfica si se solicita\n",
    "    if export_images:\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(K, inertia, 'bo-')\n",
    "        plt.xlabel('Number of clusters (k)')\n",
    "        plt.ylabel('Inertia / Metric')\n",
    "\n",
    "        # Título que incluye el algoritmo, el principio y el número óptimo de k\n",
    "        algo_name = algorithm_class.__name__\n",
    "        principle_name = principle if principle else \"Unknown_Principle\"\n",
    "        plt.title(f\"Optimum number of cluster (Algorithm: '{algo_name}', Principle: '{principle_name}', k={optimal_k})\")\n",
    "\n",
    "        # Crear el directorio \"results\" si no existe\n",
    "        results_dir = \"results\"\n",
    "        os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "        # Nombre del archivo con el algoritmo, principio y k óptimo\n",
    "        file_name = f\"{algo_name}_{principle_name}_k={optimal_k}.png\"\n",
    "        file_path = os.path.join(results_dir, file_name)\n",
    "\n",
    "        # Guardar la gráfica\n",
    "        plt.savefig(file_path, format='png')\n",
    "        plt.close()  # Cerrar la figura para liberar memoria\n",
    "        logger.warning(f\"Optimum number of clusters plot saved to: {file_path}\")\n",
    "\n",
    "    return optimal_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `find_optimal_k_elbow_method` aims to determine the optimal number of clusters (`k`) for a dataset `X` using the elbow method or equivalent metrics, depending on the clustering algorithm provided. It is compatible with multiple clustering algorithms, including KMeans, DBSCAN, OPTICS, GaussianMixture, and AgglomerativeClustering, and adapts its logic based on the features of each algorithm.\n",
    "\n",
    "The code computes a quality metric (such as inertia or silhouette score) for different values of `k`, stores the results, and analyzes the second difference in these metrics to identify the \"elbow point,\" which represents the optimal `k`. \n",
    "\n",
    "If image export is enabled, it generates and saves a plot that shows the relationship between `k` and the metric, including details about the algorithm and the analyzed principle. Additionally, it organizes the results in a directory and labels the files for easy identification.\n",
    "\n",
    "Finally, it returns the optimal `k` value found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the function to detect the optimal number of clusters is used to evaluate the optimal number of clusters for each algorithm based on each persuasion principle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "opt_k_dict = {}\n",
    "\n",
    "for principle, embed_df in embeddings.items():\n",
    "    logger.info(f\"Obtaining the optimum number of clusters for principle: '{principle}'\")\n",
    "\n",
    "    # Preprocesamiento: Escalamiento de los datos\n",
    "    X_raw = embed_df.drop(columns=[\"label\", \"class\"]).values  # Datos sin columnas \"label\" y \"class\"\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X_raw)  # Escalar los datos\n",
    "    logger.info(f\"Data shape for principle '{principle}' after scaling: {X.shape}\")\n",
    "\n",
    "    algo_k_dict = {}\n",
    "    for name, algo_class in clustering_algos.items():  # Asegurarse de que `clustering_algos` contiene clases\n",
    "        logger.info(f\"Using algorithm: {name}\")\n",
    "\n",
    "        try:\n",
    "            # Determinar el número óptimo de clusters\n",
    "            optimal_k = find_optimal_k_elbow_method(\n",
    "                X, \n",
    "                algorithm_class=algo_class, \n",
    "                principle=principle,  # Indica el principio (puedes cambiarlo)\n",
    "                export_images=True,       # Exportar la gráfica\n",
    "                max_k=10                  # Máximo número de clusters a evaluar\n",
    "            )\n",
    "\n",
    "            algo_k_dict[name] = optimal_k\n",
    "            opt_k_dict[principle] = algo_k_dict\n",
    "            logger.info(f\"Optimal number of clusters for principle '{principle}' - '{name}': {optimal_k}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"An error occurred while processing '{name}' for principle '{principle}': {e}\")\n",
    "\n",
    "logger.info(\"optimum number of clusters evaluation completed for all principles.\")\n",
    "\n",
    "# Convertir el diccionario de óptimos de k en un DataFrame\n",
    "opt_k_df = pd.DataFrame(opt_k_dict).T  # Transponer para que los principios sean las filas\n",
    "opt_k_df.index.name = \"Principle\"\n",
    "opt_k_df.reset_index(inplace=True)  # Convertir el índice en columna para mejor visualización\n",
    "\n",
    "# Mostrar la tabla de resultados\n",
    "print(\"\\nOptimal Number of Clusters (k) for Each Algorithm and Principle:\")\n",
    "print(opt_k_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the analytical method determines the optimal number of groups created, an interactive window is displayed for the analyst to confirm them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar el DataFrame en una ventana interactiva\n",
    "gui = show(opt_k_df)\n",
    "\n",
    "# Confirmar y usar los valores actualizados (después de cerrar la GUI)\n",
    "opt_k_df_updated = gui.get_dataframes()['opt_k_df']\n",
    "print(\"Updated DataFrame:\")\n",
    "print(opt_k_df_updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal number of clusters confirmed by the analyst is displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_k_df_updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal number of clusters is then stored as the user has confirmed them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"results/optimal_k_values.csv\"\n",
    "logger.info(\"Saving the optimal number of clusters to `{path}}`...\")\n",
    "opt_k_df.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the optimal number of groups has been determined, the clustering of the data is performed using the pipeline, which evaluates all clustering algorithms with the previously selected optimal number of groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una carpeta para guardar las imágenes si no existe\n",
    "output_dir = \"results\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Evaluar cada algoritmo usando un pipeline para cada principio\n",
    "results = {}\n",
    "\n",
    "for principle, embed_df in embeddings.items():\n",
    "    logger.info(f\"Evaluating clustering algorithms for principle: '{principle}'\")\n",
    "    \n",
    "    X = embed_df.drop(columns=[\"label\", \"class\"]).values  # Obtener los datos sin las columnas \"label\" y \"class\"\n",
    "    logger.info(f\"Data shape for principle '{principle}': {X.shape}\")\n",
    "\n",
    "    # Obtener las configuraciones del número óptimo de clusters para el principio actual\n",
    "    opt_k_row = opt_k_df_updated[opt_k_df_updated['Principle'] == principle]\n",
    "    if opt_k_row.empty:\n",
    "        logger.warning(f\"No optimal k configuration found for principle '{principle}'. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    for name, algo_class in clustering_algos.items():\n",
    "        logger.info(f\"Using algorithm: {name}\")\n",
    "\n",
    "        try:\n",
    "            # Crear una instancia temporal para verificar si tiene el atributo correspondiente\n",
    "            algo_instance = algo_class()\n",
    "\n",
    "            # Verificar el parámetro adecuado para el algoritmo\n",
    "            param_name = param_mapping.get(name, None)\n",
    "            \n",
    "            if param_name:  # Si el algoritmo tiene un parámetro específico para el número de clusters\n",
    "                if name in opt_k_row.columns:\n",
    "                    optimal_k = int(opt_k_row[name].values[0])  # Número óptimo de clusters\n",
    "                    logger.info(f\"Using optimal number of clusters ({optimal_k}) for algorithm: {name}\")\n",
    "                else:\n",
    "                    logger.warning(f\"Algorithm '{name}' not found in optimal k configurations. Skipping...\")\n",
    "                    continue\n",
    "\n",
    "                # Crear el pipeline con el número óptimo de clusters\n",
    "                pipeline = Pipeline([\n",
    "                    ('scaler', StandardScaler()),  # Estandarizar las características\n",
    "                    ('cluster', algo_class(**{param_name: optimal_k}))  # Configuración con el número óptimo de clusters\n",
    "                ])\n",
    "\n",
    "            else:  # Algoritmos que no tienen un parámetro para el número de clusters\n",
    "                logger.info(f\"Algorithm '{name}' does not use '{param_name}'. Proceeding with default parameters.\")\n",
    "                pipeline = Pipeline([\n",
    "                    ('scaler', StandardScaler()),  # Estandarizar las características\n",
    "                    ('cluster', algo_class())      # Configuración por defecto\n",
    "                ])\n",
    "\n",
    "            # Ajustar el pipeline a los datos\n",
    "            pipeline.fit(X)\n",
    "            logger.info(f\"Pipeline fitting completed for algorithm: {name}\")\n",
    "\n",
    "            # Obtener las etiquetas de los clusters\n",
    "            if hasattr(pipeline.named_steps['cluster'], 'labels_'):\n",
    "                labels = pipeline.named_steps['cluster'].labels_\n",
    "            else:\n",
    "                labels = pipeline.named_steps['cluster'].predict(X)\n",
    "\n",
    "            logger.info(f\"Labels obtained for algorithm: {name}\")\n",
    "\n",
    "            # Verificar el número de clusters antes de calcular la puntuación de silueta\n",
    "            n_clusters = len(np.unique(labels))\n",
    "            if n_clusters > 1:\n",
    "                # Calcular la puntuación de silueta\n",
    "                score = silhouette_score(X, labels)\n",
    "                results.setdefault(principle, {})[name] = score\n",
    "                logger.info(f\"Silhouette Score for '{name}' on principle '{principle}': {score:.4f}\")\n",
    "\n",
    "                # PCA para reducir la dimensionalidad a 2\n",
    "                pca = PCA(n_components=2)\n",
    "                X_pca = pca.fit_transform(X)\n",
    "\n",
    "                # Graficar los grupos formados\n",
    "                plt.figure(figsize=(8, 6))\n",
    "\n",
    "                # Graficar cada grupo con un color diferente\n",
    "                scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='tab20', s=50, alpha=0.7)\n",
    "\n",
    "                # Título con el nombre del principio, el algoritmo y el número de grupos\n",
    "                plt.title(f\"Clustering with {name} for Principle: {principle} (Clusters: {n_clusters})\")\n",
    "\n",
    "                # Etiquetas de los ejes\n",
    "                plt.xlabel(\"PCA Component 1\")\n",
    "                plt.ylabel(\"PCA Component 2\")\n",
    "\n",
    "                # Leyenda con los colores de los clusters\n",
    "                plt.legend(*scatter.legend_elements(), title=\"Clusters\")\n",
    "\n",
    "                # Barra de colores\n",
    "                plt.colorbar(scatter, label='Cluster ID')\n",
    "\n",
    "                # Guardar el gráfico en la carpeta de trabajo\n",
    "                output_path = os.path.join(output_dir, f\"{principle}_{name}_clusters_{n_clusters}.png\")\n",
    "                plt.savefig(output_path)\n",
    "                logger.info(f\"Saved the plot as '{output_path}'\")\n",
    "\n",
    "                # Limpiar la figura para la siguiente iteración\n",
    "                plt.clf()\n",
    "\n",
    "            else:\n",
    "                logger.warning(f\"Algorithm '{name}' generated only one cluster for principle '{principle}' and was skipped.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"An error occurred while processing '{name}' for principle '{principle}': {e}\")\n",
    "\n",
    "logger.info(\"Clustering evaluation completed for all principles.\")\n",
    "\n",
    "\n",
    "# Crear un DataFrame vacío para almacenar los resultados\n",
    "summary_df = pd.DataFrame()\n",
    "\n",
    "# Llenar el DataFrame con los resultados\n",
    "for principle, algo_scores in results.items():\n",
    "    row = {algo: score for algo, score in algo_scores.items()}\n",
    "    row['Principle'] = principle\n",
    "    # Usar pd.concat para agregar la fila al DataFrame\n",
    "    summary_df = pd.concat([summary_df, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "# Establecer la columna 'Principle' como índice\n",
    "summary_df.set_index('Principle', inplace=True)\n",
    "\n",
    "# Mostrar la tabla resumen\n",
    "print(summary_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering results\n",
    "\n",
    "-------------------------------------------------------------------\n",
    "| Principle   | KMeans    | GaussianMixture | OPTICS   | HCA      |\n",
    "|-------------|-----------|-----------------|----------|----------|\n",
    "| authority   | 0.186626  | 0.339361        | -0.141847| 0.165578 |\n",
    "| distraction | 0.082094  | 0.098757        | -0.146568| 0.094199 |\n",
    "-------------------------------------------------------------------\n",
    "\n",
    "## Interpretation of the Results:\n",
    "\n",
    "- **Silhouette Score**: The silhouette score is a measure that evaluates how well each point is within its own group (cohesion) and how far it is from other groups (separation). A silhouette score close to 1 indicates that points are well grouped, a score near 0 suggests points are on the border between two groups, and a negative score indicates that points may be poorly clustered.\n",
    "\n",
    "1. **KMeans (authority: 0.1866, distraction: 0.0821)**:\n",
    "   - **authority**: The score of 0.1866 suggests that the points are somewhat well grouped, but there is room for improvement. This is a low value for KMeans, indicating that the separation between clusters is not clear, or the clusters are too dispersed.\n",
    "   - **distraction**: With a score of 0.0821, the clustering quality is low, indicating that points within the groups are not well-defined, or the groups are too close to each other.\n",
    "\n",
    "2. **GaussianMixture (authority: 0.3394, distraction: 0.0988)**:\n",
    "   - **authority**: The score of 0.3394 is higher than KMeans, indicating better separation between clusters compared to KMeans. Although not a very high value, it shows that the groups are somewhat more coherent.\n",
    "   - **distraction**: Similar to KMeans, GaussianMixture has a low score for `distraction` (0.0988), indicating that the clustering is not strong. Points in this principle may be scattered across several groups.\n",
    "\n",
    "3. **OPTICS (authority: -0.1418, distraction: -0.1466)**:\n",
    "   - **authority**: The negative value of -0.1418 for OPTICS on the `authority` principle is concerning, as it indicates that points are poorly clustered. This suggests that OPTICS has not found well-defined groups, which could be due to inadequate configuration or a data structure that is not compatible with the density used by this algorithm.\n",
    "   - **distraction**: The score of -0.1466 is also negative, meaning that OPTICS has not successfully clustered the data in this principle. It may be that the data lacks clear density or that OPTICS parameters are not well-tuned for this type of data.\n",
    "\n",
    "4. **HCA (authority: 0.1656, distraction: 0.0942)**:\n",
    "   - **authority**: HCA has a score of 0.1656, indicating a fairly good clustering, although still not optimal. HCA works well when there is a clear hierarchical structure in the data, but if no such structure exists, the results may be less accurate.\n",
    "   - **distraction**: Similar to KMeans and GaussianMixture, the score of 0.0942 for `distraction` suggests that HCA does not achieve good clustering on this principle. Like the other algorithms, points may be dispersed across clusters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRASH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Clustering usando K-means\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)\n",
    "kmeans.fit(embeddings)\n",
    "\n",
    "# Visualización con PCA (reducción de dimensionalidad)\n",
    "pca = PCA(n_components=2)\n",
    "reduced_embeddings = pca.fit_transform(embeddings)\n",
    "\n",
    "# Crear un mapa de colores para cada cluster\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Dibujar los puntos con los colores correspondientes a sus etiquetas de cluster\n",
    "scatter = plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=kmeans.labels_, cmap='tab10')\n",
    "\n",
    "# Título y etiquetas de los ejes\n",
    "plt.title('Clustering of Texts')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "\n",
    "# Crear la leyenda manualmente para cada cluster\n",
    "handles, labels = scatter.legend_elements()\n",
    "plt.legend(handles, labels, title=\"Cluster\")\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "# Obtener los centros de los clusters\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "\n",
    "# Calcular la distancia de cada mensaje al centro del clúster al que pertenece\n",
    "distances = euclidean_distances(embeddings, cluster_centers)\n",
    "\n",
    "# Asignar la intensidad basada en la distancia\n",
    "# Normalización de la distancia a una escala de 1 a 10\n",
    "max_distance = np.max(distances)\n",
    "min_distance = np.min(distances)\n",
    "\n",
    "intensities = 10 * (1 - (distances - min_distance) / (max_distance - min_distance))\n",
    "\n",
    "\n",
    "# Crear un DataFrame con los resultados\n",
    "results = pd.DataFrame({\n",
    "    'Text': data_authority[\"txt\"].tolist(),  # Asegúrate de tener el DataFrame con tus datos\n",
    "    'Intensity': [intensities[i].max() for i in range(len(intensities))],  # Intensidad calculada\n",
    "    'Cluster': kmeans.labels_  # Clúster asignado\n",
    "})\n",
    "\n",
    "# Mostrar el DataFrame resultante\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Calcular las distancias a los centros de los clústeres\n",
    "distances_to_centers = np.linalg.norm(embeddings - kmeans.cluster_centers_[kmeans.labels_], axis=1)\n",
    "\n",
    "# Normalizar las distancias a la escala de 1 a 10\n",
    "scaler = MinMaxScaler(feature_range=(1, 10))\n",
    "distances_normalized = scaler.fit_transform(distances_to_centers.reshape(-1, 1))\n",
    "\n",
    "# Crear un DataFrame con los resultados\n",
    "results = pd.DataFrame({\n",
    "    'Text': data_authority[\"txt\"].tolist(),  # Asegúrate de tener el DataFrame con tus datos\n",
    "    'Intensity': distances_normalized.flatten(),  # Intensidad calculada\n",
    "    'Cluster': kmeans.labels_  # Clúster asignado\n",
    "})\n",
    "\n",
    "# Calcular el rango de intensidad para cada grupo\n",
    "cluster_ranges = {}\n",
    "for cluster in np.unique(kmeans.labels_):\n",
    "    # Obtener las intensidades para el clúster\n",
    "    intensities_for_cluster = results[results['Cluster'] == cluster]['Intensity']\n",
    "    min_intensity = intensities_for_cluster.min()\n",
    "    max_intensity = intensities_for_cluster.max()\n",
    "    \n",
    "    # Asignar el rango de intensidad al clúster\n",
    "    cluster_ranges[cluster] = (min_intensity, max_intensity)\n",
    "\n",
    "# Asignar el rango de intensidad a cada texto\n",
    "def get_intensity_range(cluster):\n",
    "    min_intensity, max_intensity = cluster_ranges[cluster]\n",
    "    return f\"Grupo {cluster + 1}, intensidad de {min_intensity:.2f} a {max_intensity:.2f}\"\n",
    "\n",
    "results['Intensity Range'] = results['Cluster'].apply(get_intensity_range)\n",
    "\n",
    "# Mostrar el DataFrame resultante\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular el promedio, varianza y desviación estándar para cada grupo (clúster)\n",
    "group_stats = results.groupby('Cluster')['Intensity'].agg(\n",
    "    ['mean', 'var', 'std']\n",
    ").reset_index()\n",
    "\n",
    "# Mostrar las estadísticas de los grupos\n",
    "group_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Calcular el promedio, varianza y desviación estándar para cada grupo (clúster)\n",
    "group_stats = results.groupby('Cluster')['Intensity'].agg(\n",
    "    ['mean', 'var', 'std']\n",
    ").reset_index()\n",
    "\n",
    "# Mostrar las estadísticas de los grupos\n",
    "print(group_stats)\n",
    "\n",
    "# Calcular el índice de silueta para el modelo de clustering\n",
    "sil_score = silhouette_score(embeddings, kmeans.labels_)\n",
    "print(f\"Silhouette Score: {sil_score:.3f}\")\n",
    "\n",
    "# Calcular la cohesión: distancia intra-clúster (dentro de cada clúster)\n",
    "cohesion = np.sum([np.sum(np.linalg.norm(embeddings[kmeans.labels_ == i] - kmeans.cluster_centers_[i], axis=1)) for i in range(kmeans.n_clusters)])\n",
    "\n",
    "# Calcular la separación: distancia entre los centros de los clústeres\n",
    "separation = np.sum([np.linalg.norm(kmeans.cluster_centers_[i] - kmeans.cluster_centers_[j]) for i in range(kmeans.n_clusters) for j in range(i + 1, kmeans.n_clusters)])\n",
    "\n",
    "print(f\"Cohesión: {cohesion:.2f}\")\n",
    "print(f\"Separación: {separation:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Calcular el índice de silueta para cada punto\n",
    "silhouette_values = silhouette_samples(embeddings, kmeans.labels_)\n",
    "\n",
    "# Asignar el índice de silueta promedio por grupo\n",
    "silhouette_scores = []\n",
    "for cluster in np.unique(kmeans.labels_):\n",
    "    cluster_silhouette = silhouette_values[kmeans.labels_ == cluster]\n",
    "    silhouette_scores.append((cluster, np.mean(cluster_silhouette)))\n",
    "\n",
    "# Mostrar los resultados\n",
    "silhouette_df = pd.DataFrame(silhouette_scores, columns=[\"Cluster\", \"Average Silhouette Score\"])\n",
    "print(\"Silhouette Scores by Cluster:\")\n",
    "print(silhouette_df)\n",
    "\n",
    "# Calcular la cohesión para cada clúster: distancia media entre los puntos y el centro\n",
    "cohesion_scores = []\n",
    "for cluster in np.unique(kmeans.labels_):\n",
    "    cluster_points = embeddings[kmeans.labels_ == cluster]\n",
    "    center = kmeans.cluster_centers_[cluster]\n",
    "    cohesion = np.mean(np.linalg.norm(cluster_points - center, axis=1))\n",
    "    cohesion_scores.append((cluster, cohesion))\n",
    "\n",
    "# Mostrar los resultados\n",
    "cohesion_df = pd.DataFrame(cohesion_scores, columns=[\"Cluster\", \"Cohesion\"])\n",
    "print(\"\\nCohesion Scores by Cluster:\")\n",
    "print(cohesion_df)\n",
    "\n",
    "# Calcular la separación para cada clúster: distancia promedio entre el centro del clúster y los otros centros\n",
    "separation_scores = []\n",
    "for cluster in np.unique(kmeans.labels_):\n",
    "    separation = np.mean([np.linalg.norm(kmeans.cluster_centers_[cluster] - kmeans.cluster_centers_[other_cluster]) \n",
    "                          for other_cluster in np.unique(kmeans.labels_) if other_cluster != cluster])\n",
    "    separation_scores.append((cluster, separation))\n",
    "\n",
    "# Mostrar los resultados\n",
    "separation_df = pd.DataFrame(separation_scores, columns=[\"Cluster\", \"Separation\"])\n",
    "print(\"\\nSeparation Scores by Cluster:\")\n",
    "print(separation_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Calcular el índice de silueta para cada punto\n",
    "silhouette_values = silhouette_samples(embeddings, kmeans.labels_)\n",
    "\n",
    "# Calcular el índice de silueta promedio por grupo\n",
    "silhouette_scores = []\n",
    "for cluster in np.unique(kmeans.labels_):\n",
    "    cluster_silhouette = silhouette_values[kmeans.labels_ == cluster]\n",
    "    silhouette_scores.append((cluster, np.mean(cluster_silhouette)))\n",
    "\n",
    "# Calcular la cohesión para cada clúster: distancia media entre los puntos y el centro\n",
    "cohesion_scores = []\n",
    "for cluster in np.unique(kmeans.labels_):\n",
    "    cluster_points = embeddings[kmeans.labels_ == cluster]\n",
    "    center = kmeans.cluster_centers_[cluster]\n",
    "    cohesion = np.mean(np.linalg.norm(cluster_points - center, axis=1))\n",
    "    cohesion_scores.append((cluster, cohesion))\n",
    "\n",
    "# Calcular la separación para cada clúster: distancia promedio entre el centro del clúster y los otros centros\n",
    "separation_scores = []\n",
    "for cluster in np.unique(kmeans.labels_):\n",
    "    separation = np.mean([np.linalg.norm(kmeans.cluster_centers_[cluster] - kmeans.cluster_centers_[other_cluster]) \n",
    "                          for other_cluster in np.unique(kmeans.labels_) if other_cluster != cluster])\n",
    "    separation_scores.append((cluster, separation))\n",
    "\n",
    "# Crear un DataFrame con todas las métricas\n",
    "metrics_df = pd.DataFrame({\n",
    "    \"Cluster\": np.unique(kmeans.labels_),\n",
    "    \"Average Silhouette Score\": [score[1] for score in silhouette_scores],\n",
    "    \"Cohesion\": [score[1] for score in cohesion_scores],\n",
    "    \"Separation\": [score[1] for score in separation_scores]\n",
    "})\n",
    "\n",
    "# Mostrar el DataFrame final con todas las métricas\n",
    "print(metrics_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Calcular las métricas adicionales para cada grupo\n",
    "group_metrics = []\n",
    "\n",
    "# Iterar sobre los grupos (clústeres)\n",
    "for cluster in np.unique(kmeans.labels_):\n",
    "    # Filtrar las intensidades del grupo\n",
    "    intensities_for_cluster = results[results['Cluster'] == cluster]['Intensity']\n",
    "    \n",
    "    # Calcular las métricas\n",
    "    avg_intensity = intensities_for_cluster.mean()\n",
    "    std_dev_intensity = intensities_for_cluster.std()\n",
    "    variance_intensity = intensities_for_cluster.var()\n",
    "    kurtosis_intensity = stats.kurtosis(intensities_for_cluster)\n",
    "    min_intensity = intensities_for_cluster.min()\n",
    "    max_intensity = intensities_for_cluster.max()\n",
    "    \n",
    "    # Obtener el rango de intensidad\n",
    "    intensity_range = f\"De {min_intensity:.2f} a {max_intensity:.2f}\"\n",
    "    \n",
    "    # Obtener la métrica de separación y cohesión\n",
    "    cohesion = cluster_metrics[cluster]['Cohesion']\n",
    "    separation = cluster_metrics[cluster]['Separation']\n",
    "    silhouette_score = cluster_metrics[cluster]['Silhouette']\n",
    "    \n",
    "    # Añadir las métricas al resultado final\n",
    "    group_metrics.append({\n",
    "        'Cluster': cluster,\n",
    "        'Average Silhouette Score': silhouette_score,\n",
    "        'Cohesion': cohesion,\n",
    "        'Separation': separation,\n",
    "        'Average Intensity': avg_intensity,\n",
    "        'Standard Deviation': std_dev_intensity,\n",
    "        'Variance': variance_intensity,\n",
    "        'Kurtosis': kurtosis_intensity,\n",
    "        'Intensity Range': intensity_range\n",
    "    })\n",
    "\n",
    "# Crear el DataFrame final con todas las métricas\n",
    "metrics_df = pd.DataFrame(group_metrics)\n",
    "\n",
    "# Mostrar el DataFrame con todas las métricas\n",
    "metrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Calcular las distancias a los centros de los clústeres\n",
    "distances_to_centers = np.linalg.norm(embeddings - kmeans.cluster_centers_[kmeans.labels_], axis=1)\n",
    "\n",
    "# Normalizar las distancias a la escala de 1 a 10, de forma independiente por clúster\n",
    "scaler = MinMaxScaler(feature_range=(1, 10))\n",
    "\n",
    "# Crear un DataFrame con los resultados\n",
    "results = pd.DataFrame({\n",
    "    'Text': data_authority[\"txt\"].tolist(),  # Asegúrate de tener el DataFrame con tus datos\n",
    "    'Distance': distances_to_centers,  # Distancia al centro\n",
    "    'Cluster': kmeans.labels_  # Clúster asignado\n",
    "})\n",
    "\n",
    "# Normalizar la distancia dentro de cada clúster\n",
    "for cluster in np.unique(kmeans.labels_):\n",
    "    cluster_distances = results[results['Cluster'] == cluster]['Distance'].values\n",
    "    results.loc[results['Cluster'] == cluster, 'Intensity'] = scaler.fit_transform(cluster_distances.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Calcular el rango de intensidad para cada grupo\n",
    "cluster_ranges = {}\n",
    "for cluster in np.unique(kmeans.labels_):\n",
    "    # Obtener las intensidades para el clúster\n",
    "    intensities_for_cluster = results[results['Cluster'] == cluster]['Intensity']\n",
    "    min_intensity = intensities_for_cluster.min()\n",
    "    max_intensity = intensities_for_cluster.max()\n",
    "    \n",
    "    # Asignar el rango de intensidad al clúster\n",
    "    cluster_ranges[cluster] = (min_intensity, max_intensity)\n",
    "\n",
    "# Asignar el rango de intensidad a cada texto\n",
    "def get_intensity_range(cluster):\n",
    "    min_intensity, max_intensity = cluster_ranges[cluster]\n",
    "    return f\"Grupo {cluster + 1}, intensidad de {min_intensity:.2f} a {max_intensity:.2f}\"\n",
    "\n",
    "results['Intensity Range'] = results['Cluster'].apply(get_intensity_range)\n",
    "\n",
    "# Mostrar el DataFrame resultante\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Calcular las distancias a los centros de los clústeres\n",
    "distances_to_centers = np.linalg.norm(embeddings - kmeans.cluster_centers_[kmeans.labels_], axis=1)\n",
    "\n",
    "# Crear un DataFrame con los resultados\n",
    "results = pd.DataFrame({\n",
    "    'Text': data_authority[\"txt\"].tolist(),  # Asegúrate de tener el DataFrame con tus datos\n",
    "    'Distance': distances_to_centers,  # Distancia al centro\n",
    "    'Cluster': kmeans.labels_  # Clúster asignado\n",
    "})\n",
    "\n",
    "# Normalizar la distancia dentro de cada clúster, sin afectar a los demás\n",
    "scaler = MinMaxScaler(feature_range=(1, 10))\n",
    "\n",
    "# Calcular la intensidad dentro de cada grupo de forma independiente\n",
    "for cluster in np.unique(kmeans.labels_):\n",
    "    cluster_distances = results[results['Cluster'] == cluster]['Distance'].values\n",
    "    # Normalizamos solo las distancias del clúster actual\n",
    "    normalized_distances = scaler.fit_transform(cluster_distances.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Asignamos la intensidad normalizada al DataFrame\n",
    "    results.loc[results['Cluster'] == cluster, 'Intensity'] = normalized_distances\n",
    "\n",
    "# Calcular el rango de intensidad para cada grupo\n",
    "cluster_ranges = {}\n",
    "for cluster in np.unique(kmeans.labels_):\n",
    "    # Obtener las intensidades para el clúster\n",
    "    intensities_for_cluster = results[results['Cluster'] == cluster]['Intensity']\n",
    "    min_intensity = intensities_for_cluster.min()\n",
    "    max_intensity = intensities_for_cluster.max()\n",
    "    \n",
    "    # Asignar el rango de intensidad al clúster\n",
    "    cluster_ranges[cluster] = (min_intensity, max_intensity)\n",
    "\n",
    "# Asignar el rango de intensidad a cada texto\n",
    "def get_intensity_range(cluster):\n",
    "    min_intensity, max_intensity = cluster_ranges[cluster]\n",
    "    return f\"Grupo {cluster + 1}, intensidad de {min_intensity:.2f} a {max_intensity:.2f}\"\n",
    "\n",
    "results['Intensity Range'] = results['Cluster'].apply(get_intensity_range)\n",
    "\n",
    "# Mostrar el DataFrame resultante\n",
    "print(results[['Text', 'Intensity', 'Intensity Range']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import OPTICS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Aplicar OPTICS al conjunto de embeddings\n",
    "optics = OPTICS(min_samples=2, xi=0.05, min_cluster_size=0.1)\n",
    "optics.fit(embeddings)\n",
    "\n",
    "# Ver los resultados de los clusters\n",
    "labels = optics.labels_\n",
    "\n",
    "# Imprimir los clusters asignados a cada mensaje\n",
    "for i, label in enumerate(labels):\n",
    "    print(f\"Mensaje {i+1} - Cluster: {label}\")\n",
    "\n",
    "# Visualizar el orden de accesibilidad (alcanzabilidad)\n",
    "plt.plot(optics.reachability_, marker='o')\n",
    "plt.title(\"Reachability Plot\")\n",
    "plt.xlabel(\"Points\")\n",
    "plt.ylabel(\"Reachability Distance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Reducir a 2 dimensiones para visualización\n",
    "pca = PCA(n_components=2)\n",
    "reduced_embeddings = pca.fit_transform(embeddings)\n",
    "\n",
    "# Graficar los clusters\n",
    "plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=labels)\n",
    "plt.title(\"Clustering of Messages\")\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "plt.colorbar(label=\"Cluster\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "# Aplicar DBSCAN\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=2)\n",
    "dbscan.fit(embeddings)\n",
    "\n",
    "# Visualización con PCA (reducción de dimensionalidad)\n",
    "pca = PCA(n_components=2)\n",
    "reduced_embeddings = pca.fit_transform(embeddings)\n",
    "\n",
    "plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=dbscan.labels_)\n",
    "plt.title('DBSCAN Clustering of Texts')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "\n",
    "# Añadir leyenda\n",
    "plt.legend(handles=[plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=plt.cm.tab10(i / len(set(dbscan.labels_))), markersize=10) for i in set(dbscan.labels_)], labels=[f'Cluster {i}' for i in set(dbscan.labels_)])\n",
    "plt.show()\n",
    "\n",
    "# Asignación de intensidad según el cluster\n",
    "for i, text in enumerate(data_authority[\"txt\"].tolist()):\n",
    "    print(f\"Text: {text} - Intensity Cluster: {dbscan.labels_[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Aplicar GMM\n",
    "num_clusters = 10\n",
    "gmm = GaussianMixture(n_components=num_clusters, random_state=42)  # Puedes cambiar el número de componentes\n",
    "gmm.fit(embeddings)\n",
    "\n",
    "# Visualización con PCA (reducción de dimensionalidad)\n",
    "pca = PCA(n_components=2)\n",
    "reduced_embeddings = pca.fit_transform(embeddings)\n",
    "\n",
    "plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=gmm.predict(embeddings))\n",
    "plt.title('GMM Clustering of Texts')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "\n",
    "# Añadir leyenda\n",
    "plt.legend(handles=[plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=plt.cm.tab10(i / num_clusters), markersize=10) for i in range(num_clusters)], labels=[f'Cluster {i}' for i in range(num_clusters)])\n",
    "plt.show()\n",
    "\n",
    "# Asignación de intensidad según el cluster\n",
    "for i, text in enumerate(data_authority[\"txt\"].tolist()):\n",
    "    print(f\"Text: {text} - Intensity Cluster: {gmm.predict(embeddings)[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisis uno or uno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMEAns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Suponiendo que ya tienes tus embeddings y datos cargados en 'embeddings' y 'data_authority'\n",
    "\n",
    "# 1. Calcular las distancias a los centros de los clústeres\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)\n",
    "kmeans.fit(embeddings)\n",
    "\n",
    "distances_to_centers = np.linalg.norm(embeddings - kmeans.cluster_centers_[kmeans.labels_], axis=1)\n",
    "\n",
    "# 2. Normalizar las distancias a la escala de 1 a 10\n",
    "scaler = MinMaxScaler(feature_range=(1, 10))\n",
    "distances_normalized = scaler.fit_transform(distances_to_centers.reshape(-1, 1))\n",
    "\n",
    "# 3. Crear un DataFrame con los resultados de las intensidades\n",
    "results = pd.DataFrame({\n",
    "    'Text': data_authority[\"txt\"].tolist(),  # Asegúrate de tener el DataFrame con tus datos\n",
    "    'Intensity': distances_normalized.flatten(),\n",
    "    'Cluster': kmeans.labels_\n",
    "})\n",
    "\n",
    "# 4. Calcular el rango de intensidad para cada grupo\n",
    "cluster_ranges = {}\n",
    "for cluster in np.unique(kmeans.labels_):\n",
    "    intensities_for_cluster = results[results['Cluster'] == cluster]['Intensity']\n",
    "    min_intensity = intensities_for_cluster.min()\n",
    "    max_intensity = intensities_for_cluster.max()\n",
    "    cluster_ranges[cluster] = (min_intensity, max_intensity)\n",
    "\n",
    "# 5. Asignar el rango de intensidad a cada texto\n",
    "def get_intensity_range(cluster):\n",
    "    min_intensity, max_intensity = cluster_ranges[cluster]\n",
    "    return f\"Grupo {cluster + 1}, intensidad de {min_intensity:.2f} a {max_intensity:.2f}\"\n",
    "\n",
    "results['Intensity Range'] = results['Cluster'].apply(get_intensity_range)\n",
    "\n",
    "# 6. Calcular las métricas de calidad del agrupamiento\n",
    "cohesion_scores = {}\n",
    "separation_scores = {}\n",
    "silhouette_scores = {}\n",
    "\n",
    "for cluster in np.unique(kmeans.labels_):\n",
    "    # Calcular cohesión\n",
    "    cluster_data = embeddings[kmeans.labels_ == cluster]\n",
    "    cohesion = np.mean(np.linalg.norm(cluster_data - np.mean(cluster_data, axis=0), axis=1))\n",
    "    cohesion_scores[cluster] = cohesion\n",
    "    \n",
    "    # Calcular separación\n",
    "    other_clusters_data = embeddings[kmeans.labels_ != cluster]\n",
    "    separation = np.mean(np.linalg.norm(cluster_data - other_clusters_data.mean(axis=0), axis=1))\n",
    "    separation_scores[cluster] = separation\n",
    "    \n",
    "    # Calcular la puntuación de Silhouette\n",
    "    silhouette = silhouette_score(embeddings, kmeans.labels_)\n",
    "    silhouette_scores[cluster] = silhouette\n",
    "\n",
    "# 7. Calcular las métricas adicionales (promedio, desviación estándar, varianza, kurtosis)\n",
    "group_metrics = []\n",
    "\n",
    "for cluster in np.unique(kmeans.labels_):\n",
    "    # Filtrar las intensidades del grupo\n",
    "    intensities_for_cluster = results[results['Cluster'] == cluster]['Intensity']\n",
    "    \n",
    "    # Calcular las métricas\n",
    "    avg_intensity = intensities_for_cluster.mean()\n",
    "    std_dev_intensity = intensities_for_cluster.std()\n",
    "    variance_intensity = intensities_for_cluster.var()\n",
    "    kurtosis_intensity = stats.kurtosis(intensities_for_cluster)\n",
    "    min_intensity = intensities_for_cluster.min()\n",
    "    max_intensity = intensities_for_cluster.max()\n",
    "    \n",
    "    # Obtener el rango de intensidad\n",
    "    intensity_range = f\"De {min_intensity:.2f} a {max_intensity:.2f}\"\n",
    "    \n",
    "    # Obtener las métricas de calidad del agrupamiento\n",
    "    silhouette_score_value = silhouette_scores[cluster]\n",
    "    cohesion = cohesion_scores[cluster]\n",
    "    separation = separation_scores[cluster]\n",
    "    \n",
    "    # Añadir todas las métricas al resultado final\n",
    "    group_metrics.append({\n",
    "        'Cluster': cluster,\n",
    "        'Average Silhouette Score': silhouette_score_value,\n",
    "        'Cohesion': cohesion,\n",
    "        'Separation': separation,\n",
    "        'Average Intensity': avg_intensity,\n",
    "        'Standard Deviation': std_dev_intensity,\n",
    "        'Variance': variance_intensity,\n",
    "        'Kurtosis': kurtosis_intensity,\n",
    "        'Intensity Range': intensity_range\n",
    "    })\n",
    "\n",
    "# 8. Crear el DataFrame final con todas las métricas\n",
    "metrics_df = pd.DataFrame(group_metrics)\n",
    "\n",
    "# Mostrar el DataFrame con todas las métricas\n",
    "metrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy.stats as stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Suponiendo que ya tienes tus embeddings y datos cargados en 'embeddings' y 'data_authority'\n",
    "\n",
    "# 1. Definir el pipeline para el procesamiento de datos\n",
    "scaler = StandardScaler()\n",
    "pca = PCA(n_components=2)  # Si deseas usar PCA para reducción de dimensionalidad\n",
    "\n",
    "# 2. Lista de algoritmos de agrupamiento a probar\n",
    "clustering_algorithms = {\n",
    "    'KMeans': KMeans(n_clusters=10, random_state=42),\n",
    "    'DBSCAN': DBSCAN(eps=0.5, min_samples=5),\n",
    "    'OPTICS': OPTICS(min_samples=5, xi=0.05),\n",
    "    'GMM': GaussianMixture(n_components=10, random_state=42)\n",
    "}\n",
    "\n",
    "# 3. Función para calcular las métricas de calidad del agrupamiento\n",
    "def evaluate_clustering_algorithm(algorithm, embeddings, data_authority):\n",
    "    # Ajustar el algoritmo al conjunto de datos\n",
    "    algorithm.fit(embeddings)\n",
    "    \n",
    "    # Obtener las etiquetas de los clústeres y las distancias a los centros\n",
    "    labels = algorithm.labels_ if hasattr(algorithm, 'labels_') else None\n",
    "    if labels is None:\n",
    "        labels = algorithm.predict(embeddings)  # En caso de GMM, usamos 'predict' para obtener las etiquetas\n",
    "    \n",
    "    # 4. Calcular las distancias a los centros de los clústeres (para KMeans)\n",
    "    if isinstance(algorithm, KMeans):\n",
    "        distances_to_centers = np.linalg.norm(embeddings - algorithm.cluster_centers_[labels], axis=1)\n",
    "    else:\n",
    "        distances_to_centers = np.linalg.norm(embeddings - np.mean(embeddings, axis=0), axis=1)  # Aproximación\n",
    "\n",
    "    # 5. Normalizar las distancias a la escala de 1 a 10\n",
    "    scaler = MinMaxScaler(feature_range=(1, 10))\n",
    "    distances_normalized = scaler.fit_transform(distances_to_centers.reshape(-1, 1))\n",
    "\n",
    "    # 6. Crear un DataFrame con los resultados de las intensidades\n",
    "    results = pd.DataFrame({\n",
    "        'Text': data_authority[\"txt\"].tolist(),\n",
    "        'Intensity': distances_normalized.flatten(),\n",
    "        'Cluster': labels\n",
    "    })\n",
    "\n",
    "    # 7. Calcular las métricas de calidad del agrupamiento\n",
    "    cluster_metrics = []\n",
    "    for cluster in np.unique(labels):\n",
    "        intensities_for_cluster = results[results['Cluster'] == cluster]['Intensity']\n",
    "        \n",
    "        # Calcular las métricas\n",
    "        avg_intensity = intensities_for_cluster.mean()\n",
    "        std_dev_intensity = intensities_for_cluster.std()\n",
    "        variance_intensity = intensities_for_cluster.var()\n",
    "        kurtosis_intensity = stats.kurtosis(intensities_for_cluster)\n",
    "        min_intensity = intensities_for_cluster.min()\n",
    "        max_intensity = intensities_for_cluster.max()\n",
    "\n",
    "        # Obtener el rango de intensidad\n",
    "        intensity_range = f\"De {min_intensity:.2f} a {max_intensity:.2f}\"\n",
    "\n",
    "        # Calcular cohesión, separación y silueta (usando silhouette_score)\n",
    "        cohesion = np.mean(np.linalg.norm(embeddings[labels == cluster] - np.mean(embeddings[labels == cluster], axis=0), axis=1))\n",
    "        other_clusters_data = embeddings[labels != cluster]\n",
    "        separation = np.mean(np.linalg.norm(embeddings[labels == cluster] - other_clusters_data.mean(axis=0), axis=1))\n",
    "        silhouette_score_value = silhouette_score(embeddings, labels)\n",
    "\n",
    "        # Añadir las métricas al grupo\n",
    "        cluster_metrics.append({\n",
    "            'Cluster': cluster,\n",
    "            'Average Silhouette Score': silhouette_score_value,\n",
    "            'Cohesion': cohesion,\n",
    "            'Separation': separation,\n",
    "            'Average Intensity': avg_intensity,\n",
    "            'Standard Deviation': std_dev_intensity,\n",
    "            'Variance': variance_intensity,\n",
    "            'Kurtosis': kurtosis_intensity,\n",
    "            'Intensity Range': intensity_range\n",
    "        })\n",
    "\n",
    "    # Crear el DataFrame final con todas las métricas\n",
    "    metrics_df = pd.DataFrame(cluster_metrics)\n",
    "    \n",
    "    return metrics_df\n",
    "\n",
    "# 8. Evaluar cada algoritmo de agrupamiento\n",
    "all_metrics = {}\n",
    "\n",
    "for name, algorithm in clustering_algorithms.items():\n",
    "    print(f\"Evaluando el algoritmo {name}\")\n",
    "    metrics_df = evaluate_clustering_algorithm(algorithm, embeddings, data_authority)\n",
    "    all_metrics[name] = metrics_df\n",
    "\n",
    "# Mostrar las métricas de todos los algoritmos\n",
    "for name, metrics_df in all_metrics.items():\n",
    "    print(f\"\\nMétricas para {name}:\")\n",
    "    print(metrics_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy.stats as stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "# Suponiendo que ya tienes tus embeddings y datos cargados en 'embeddings' y 'data_authority'\n",
    "\n",
    "# 1. Definir el pipeline para el procesamiento de datos\n",
    "scaler = StandardScaler()\n",
    "pca = PCA(n_components=2)  # Si deseas usar PCA para reducción de dimensionalidad\n",
    "\n",
    "# 2. Lista de algoritmos de agrupamiento a probar\n",
    "clustering_algorithms = {\n",
    "    'KMeans': KMeans(n_clusters=10, random_state=42),\n",
    "    'DBSCAN': DBSCAN(eps=0.5, min_samples=5),\n",
    "    'OPTICS': OPTICS(min_samples=5, xi=0.05),\n",
    "    'GMM': GaussianMixture(n_components=10, random_state=42)\n",
    "}\n",
    "\n",
    "# 3. Función para calcular las métricas de calidad del agrupamiento\n",
    "def evaluate_clustering_algorithm(algorithm, embeddings, data_authority):\n",
    "    # Ajustar el algoritmo al conjunto de datos\n",
    "    algorithm.fit(embeddings)\n",
    "    \n",
    "    # Obtener las etiquetas de los clústeres y las distancias a los centros\n",
    "    labels = algorithm.labels_ if hasattr(algorithm, 'labels_') else None\n",
    "    if labels is None:\n",
    "        labels = algorithm.predict(embeddings)  # En caso de GMM, usamos 'predict' para obtener las etiquetas\n",
    "    \n",
    "    # 4. Calcular las distancias a los centros de los clústeres (para KMeans)\n",
    "    if isinstance(algorithm, KMeans):\n",
    "        distances_to_centers = np.linalg.norm(embeddings - algorithm.cluster_centers_[labels], axis=1)\n",
    "    else:\n",
    "        distances_to_centers = np.linalg.norm(embeddings - np.mean(embeddings, axis=0), axis=1)  # Aproximación\n",
    "\n",
    "    # 5. Normalizar las distancias a la escala de 1 a 10\n",
    "    scaler = MinMaxScaler(feature_range=(1, 10))\n",
    "    distances_normalized = scaler.fit_transform(distances_to_centers.reshape(-1, 1))\n",
    "\n",
    "    # 6. Crear un DataFrame con los resultados de las intensidades\n",
    "    results = pd.DataFrame({\n",
    "        'Text': data_authority[\"txt\"].tolist(),\n",
    "        'Intensity': distances_normalized.flatten(),\n",
    "        'Cluster': labels\n",
    "    })\n",
    "\n",
    "    # 7. Calcular las métricas de calidad del agrupamiento\n",
    "    cluster_metrics = []\n",
    "    for cluster in np.unique(labels):\n",
    "        intensities_for_cluster = results[results['Cluster'] == cluster]['Intensity']\n",
    "        \n",
    "        # Calcular las métricas\n",
    "        avg_intensity = intensities_for_cluster.mean()\n",
    "        std_dev_intensity = intensities_for_cluster.std()\n",
    "        variance_intensity = intensities_for_cluster.var()\n",
    "        kurtosis_intensity = stats.kurtosis(intensities_for_cluster)\n",
    "        min_intensity = intensities_for_cluster.min()\n",
    "        max_intensity = intensities_for_cluster.max()\n",
    "\n",
    "        # Obtener el rango de intensidad\n",
    "        intensity_range = f\"De {min_intensity:.2f} a {max_intensity:.2f}\"\n",
    "\n",
    "        # Calcular cohesión, separación y silueta (usando silhouette_score)\n",
    "        cohesion = np.mean(np.linalg.norm(embeddings[labels == cluster] - np.mean(embeddings[labels == cluster], axis=0), axis=1))\n",
    "        other_clusters_data = embeddings[labels != cluster]\n",
    "        separation = np.mean(np.linalg.norm(embeddings[labels == cluster] - other_clusters_data.mean(axis=0), axis=1))\n",
    "        silhouette_score_value = silhouette_score(embeddings, labels)\n",
    "\n",
    "        # Añadir las métricas al grupo\n",
    "        cluster_metrics.append({\n",
    "            'Cluster': cluster,\n",
    "            'Average Silhouette Score': silhouette_score_value,\n",
    "            'Cohesion': cohesion,\n",
    "            'Separation': separation,\n",
    "            'Average Intensity': avg_intensity,\n",
    "            'Standard Deviation': std_dev_intensity,\n",
    "            'Variance': variance_intensity,\n",
    "            'Kurtosis': kurtosis_intensity,\n",
    "            'Intensity Range': intensity_range\n",
    "        })\n",
    "\n",
    "    # Crear el DataFrame final con todas las métricas\n",
    "    metrics_df = pd.DataFrame(cluster_metrics)\n",
    "    \n",
    "    return metrics_df\n",
    "\n",
    "# 8. Evaluar cada algoritmo de agrupamiento\n",
    "all_metrics = {}\n",
    "\n",
    "for name, algorithm in clustering_algorithms.items():\n",
    "    print(f\"\\nEvaluando el algoritmo {name}\")\n",
    "    metrics_df = evaluate_clustering_algorithm(algorithm, embeddings, data_authority)\n",
    "    all_metrics[name] = metrics_df\n",
    "\n",
    "# Mostrar las métricas de todos los algoritmos usando PrettyTable\n",
    "for name, metrics_df in all_metrics.items():\n",
    "    print(f\"\\nMétricas para {name}:\")\n",
    "\n",
    "    # Crear la tabla con PrettyTable\n",
    "    table = PrettyTable()\n",
    "    table.field_names = metrics_df.columns.tolist()\n",
    "\n",
    "    # Agregar filas de datos a la tabla\n",
    "    for row in metrics_df.values:\n",
    "        table.add_row(row)\n",
    "\n",
    "    # Mostrar la tabla en la consola\n",
    "    print(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import OPTICS\n",
    "from sklearn.metrics import silhouette_score, pairwise_distances\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import scipy.stats as stats\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "# Suponiendo que ya tienes tus embeddings y datos cargados en 'embeddings' y 'data_authority'\n",
    "\n",
    "# 1. Definir el pipeline para el procesamiento de datos\n",
    "scaler = StandardScaler()\n",
    "pca = PCA(n_components=2)  # Si deseas usar PCA para reducción de dimensionalidad\n",
    "\n",
    "# 2. Lista de algoritmos de agrupamiento a probar\n",
    "clustering_algorithms = {\n",
    "    'KMeans': KMeans(n_clusters=10, random_state=42),\n",
    "    'DBSCAN': DBSCAN(eps=0.5, min_samples=5),\n",
    "    'OPTICS': OPTICS(min_samples=5, xi=0.05),\n",
    "    'GMM': GaussianMixture(n_components=10, random_state=42)\n",
    "}\n",
    "\n",
    "# 3. Función para calcular las métricas de calidad del agrupamiento\n",
    "def evaluate_clustering_algorithm(algorithm, embeddings, data_authority):\n",
    "    # Ajustar el algoritmo al conjunto de datos\n",
    "    algorithm.fit(embeddings)\n",
    "    \n",
    "    # Obtener las etiquetas de los clústeres y las distancias a los centros\n",
    "    if isinstance(algorithm, OPTICS):\n",
    "        # OPTICS no tiene un atributo 'labels_' hasta después de ajustar el ordenamiento\n",
    "        labels = algorithm.labels_\n",
    "        if len(set(labels)) == 1:  # Si solo hay un único grupo (ruido), se puede omitir la evaluación\n",
    "            return pd.DataFrame({'Cluster': [], 'Cohesion': [], 'Separation': [], 'Quality Score': []})\n",
    "    else:\n",
    "        labels = algorithm.labels_ if hasattr(algorithm, 'labels_') else None\n",
    "        if labels is None:\n",
    "            labels = algorithm.predict(embeddings)  # En caso de GMM, usamos 'predict' para obtener las etiquetas\n",
    "    \n",
    "    # 4. Calcular las distancias a los centros de los clústeres (para KMeans)\n",
    "    if isinstance(algorithm, KMeans):\n",
    "        distances_to_centers = np.linalg.norm(embeddings - algorithm.cluster_centers_[labels], axis=1)\n",
    "    else:\n",
    "        distances_to_centers = np.linalg.norm(embeddings - np.mean(embeddings, axis=0), axis=1)  # Aproximación\n",
    "\n",
    "    # 5. Normalizar las distancias a la escala de 1 a 10\n",
    "    scaler = MinMaxScaler(feature_range=(1, 10))\n",
    "    distances_normalized = scaler.fit_transform(distances_to_centers.reshape(-1, 1))\n",
    "\n",
    "    # 6. Crear un DataFrame con los resultados de las intensidades\n",
    "    results = pd.DataFrame({\n",
    "        'Text': data_authority[\"txt\"].tolist(),\n",
    "        'Intensity': distances_normalized.flatten(),\n",
    "        'Cluster': labels\n",
    "    })\n",
    "\n",
    "    # 7. Evaluar la calidad individual de cada clúster (cohesión, separación y puntuación de calidad)\n",
    "    def evaluate_cluster_quality(embeddings, labels):\n",
    "        cluster_quality = []\n",
    "        for cluster in np.unique(labels):\n",
    "            if cluster == -1:  # Si el clúster es ruido (solo para DBSCAN y OPTICS)\n",
    "                continue\n",
    "            \n",
    "            # Puntos dentro del clúster\n",
    "            cluster_points = embeddings[labels == cluster]\n",
    "\n",
    "            # Cohesión: Promedio de las distancias entre puntos dentro del clúster\n",
    "            cohesion = np.mean(pairwise_distances(cluster_points))\n",
    "\n",
    "            # Separación: Promedio de las distancias entre el clúster y otros clústeres\n",
    "            other_clusters_points = embeddings[labels != cluster]\n",
    "            separation = np.mean(pairwise_distances(cluster_points, other_clusters_points))\n",
    "\n",
    "            # Índice de calidad\n",
    "            quality = separation / cohesion\n",
    "\n",
    "            cluster_quality.append({\n",
    "                'Cluster': cluster,\n",
    "                'Cohesion': cohesion,\n",
    "                'Separation': separation,\n",
    "                'Quality Score': quality\n",
    "            })\n",
    "\n",
    "        # Crear un DataFrame con los resultados\n",
    "        cluster_quality_df = pd.DataFrame(cluster_quality)\n",
    "        return cluster_quality_df\n",
    "\n",
    "    # 8. Evaluar la calidad del agrupamiento por cada clúster\n",
    "    cluster_quality_df = evaluate_cluster_quality(embeddings, labels)\n",
    "    \n",
    "    return cluster_quality_df\n",
    "\n",
    "# 9. Evaluar cada algoritmo de agrupamiento\n",
    "all_cluster_qualities = {}\n",
    "\n",
    "for name, algorithm in clustering_algorithms.items():\n",
    "    print(f\"Evaluando el algoritmo {name}\")\n",
    "    metrics_df = evaluate_clustering_algorithm(algorithm, embeddings, data_authority)\n",
    "    all_cluster_qualities[name] = metrics_df\n",
    "\n",
    "# Función para imprimir las métricas usando PrettyTable\n",
    "def print_pretty_table(metrics_df, algorithm_name):\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"Cluster\", \"Cohesion\", \"Separation\", \"Quality Score\"]\n",
    "\n",
    "    for _, row in metrics_df.iterrows():\n",
    "        table.add_row([row[\"Cluster\"], round(row[\"Cohesion\"], 2), round(row[\"Separation\"], 2), round(row[\"Quality Score\"], 2)])\n",
    "\n",
    "    print(f\"\\nMétricas de calidad para {algorithm_name}:\")\n",
    "    print(table)\n",
    "\n",
    "# Mostrar las métricas de calidad individual para cada algoritmo con PrettyTable\n",
    "for name, quality_df in all_cluster_qualities.items():\n",
    "    if not quality_df.empty:  # Solo mostrar la tabla si hay datos\n",
    "        print_pretty_table(quality_df, name)\n",
    "    else:\n",
    "        print(f\"No se generaron clústeres válidos para {name} (posiblemente solo ruido).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy.stats as stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "# Suponiendo que ya tienes tus embeddings y datos cargados en 'embeddings' y 'data_authority'\n",
    "\n",
    "# 1. Definir el pipeline para el procesamiento de datos\n",
    "scaler = StandardScaler()\n",
    "pca = PCA(n_components=2)  # Si deseas usar PCA para reducción de dimensionalidad\n",
    "\n",
    "# 2. Lista de algoritmos de agrupamiento a probar\n",
    "clustering_algorithms = {\n",
    "    'KMeans': KMeans(n_clusters=10, random_state=42),\n",
    "    'DBSCAN': DBSCAN(eps=0.5, min_samples=5),\n",
    "    'OPTICS': OPTICS(min_samples=5, xi=0.05),\n",
    "    'GMM': GaussianMixture(n_components=10, random_state=42)\n",
    "}\n",
    "\n",
    "# 3. Función para calcular las métricas de calidad del agrupamiento\n",
    "def evaluate_clustering_algorithm(algorithm, embeddings, data_authority):\n",
    "    # Ajustar el algoritmo al conjunto de datos\n",
    "    algorithm.fit(embeddings)\n",
    "    \n",
    "    # Obtener las etiquetas de los clústeres y las distancias a los centros\n",
    "    labels = algorithm.labels_ if hasattr(algorithm, 'labels_') else None\n",
    "    if labels is None:\n",
    "        labels = algorithm.predict(embeddings)  # En caso de GMM, usamos 'predict' para obtener las etiquetas\n",
    "    \n",
    "    # 4. Calcular las distancias a los centros de los clústeres (para KMeans)\n",
    "    if isinstance(algorithm, KMeans):\n",
    "        distances_to_centers = np.linalg.norm(embeddings - algorithm.cluster_centers_[labels], axis=1)\n",
    "    else:\n",
    "        distances_to_centers = np.linalg.norm(embeddings - np.mean(embeddings, axis=0), axis=1)  # Aproximación\n",
    "\n",
    "    # 5. Normalizar las distancias a la escala de 1 a 10\n",
    "    scaler = MinMaxScaler(feature_range=(1, 10))\n",
    "    distances_normalized = scaler.fit_transform(distances_to_centers.reshape(-1, 1))\n",
    "\n",
    "    # 6. Crear un DataFrame con los resultados de las intensidades\n",
    "    results = pd.DataFrame({\n",
    "        'Text': data_authority[\"txt\"].tolist(),\n",
    "        'Intensity': distances_normalized.flatten(),\n",
    "        'Cluster': labels\n",
    "    })\n",
    "\n",
    "    # 7. Calcular las métricas de calidad del agrupamiento\n",
    "    cluster_metrics = []\n",
    "    for cluster in np.unique(labels):\n",
    "        intensities_for_cluster = results[results['Cluster'] == cluster]['Intensity']\n",
    "        \n",
    "        # Calcular las métricas\n",
    "        avg_intensity = intensities_for_cluster.mean()\n",
    "        std_dev_intensity = intensities_for_cluster.std()\n",
    "        variance_intensity = intensities_for_cluster.var()\n",
    "        kurtosis_intensity = stats.kurtosis(intensities_for_cluster)\n",
    "        min_intensity = intensities_for_cluster.min()\n",
    "        max_intensity = intensities_for_cluster.max()\n",
    "\n",
    "        # Obtener el rango de intensidad\n",
    "        intensity_range = f\"De {min_intensity:.2f} a {max_intensity:.2f}\"\n",
    "\n",
    "        # Calcular cohesión, separación y silueta (usando silhouette_score)\n",
    "        cohesion = np.mean(np.linalg.norm(embeddings[labels == cluster] - np.mean(embeddings[labels == cluster], axis=0), axis=1))\n",
    "        other_clusters_data = embeddings[labels != cluster]\n",
    "        separation = np.mean(np.linalg.norm(embeddings[labels == cluster] - other_clusters_data.mean(axis=0), axis=1))\n",
    "        silhouette_score_value = silhouette_score(embeddings, labels)\n",
    "\n",
    "        # Cálculo del Quality Score (ejemplo basado en un ponderado simple)\n",
    "        quality_score = (cohesion + separation + silhouette_score_value) / 3  # Ajusta la fórmula según lo que desees\n",
    "\n",
    "        # Añadir las métricas al grupo\n",
    "        cluster_metrics.append({\n",
    "            'Cluster': cluster,\n",
    "            'Average Silhouette Score': silhouette_score_value,\n",
    "            'Cohesion': cohesion,\n",
    "            'Separation': separation,\n",
    "            'Average Intensity': avg_intensity,\n",
    "            'Standard Deviation': std_dev_intensity,\n",
    "            'Variance': variance_intensity,\n",
    "            'Kurtosis': kurtosis_intensity,\n",
    "            'Intensity Range': intensity_range,\n",
    "            'Quality Score': quality_score\n",
    "        })\n",
    "\n",
    "    # Crear el DataFrame final con todas las métricas\n",
    "    metrics_df = pd.DataFrame(cluster_metrics)\n",
    "    \n",
    "    return metrics_df\n",
    "\n",
    "# 8. Evaluar cada algoritmo de agrupamiento\n",
    "all_metrics = {}\n",
    "\n",
    "for name, algorithm in clustering_algorithms.items():\n",
    "    print(f\"\\nEvaluando el algoritmo {name}\")\n",
    "    metrics_df = evaluate_clustering_algorithm(algorithm, embeddings, data_authority)\n",
    "    all_metrics[name] = metrics_df\n",
    "\n",
    "# Mostrar las métricas de todos los algoritmos usando PrettyTable\n",
    "for name, metrics_df in all_metrics.items():\n",
    "    print(f\"\\nMétricas para {name}:\")\n",
    "\n",
    "    # Crear la tabla con PrettyTable\n",
    "    table = PrettyTable()\n",
    "    table.field_names = metrics_df.columns.tolist()\n",
    "\n",
    "    # Agregar filas de datos a la tabla\n",
    "    for row in metrics_df.values:\n",
    "        table.add_row(row)\n",
    "\n",
    "    # Mostrar la tabla en la consola\n",
    "    print(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy.stats as stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "# Suponiendo que ya tienes tus embeddings y datos cargados en 'embeddings' y 'data_authority'\n",
    "\n",
    "# 1. Definir el pipeline para el procesamiento de datos\n",
    "scaler = StandardScaler()\n",
    "pca = PCA(n_components=2)  # Si deseas usar PCA para reducción de dimensionalidad\n",
    "\n",
    "# 2. Lista de algoritmos de agrupamiento a probar\n",
    "clustering_algorithms = {\n",
    "    'KMeans': KMeans(n_clusters=10, random_state=42),\n",
    "    'DBSCAN': DBSCAN(eps=0.5, min_samples=5),\n",
    "    'OPTICS': OPTICS(min_samples=5, xi=0.05),\n",
    "    'GMM': GaussianMixture(n_components=10, random_state=42)\n",
    "}\n",
    "\n",
    "# 3. Función para calcular las métricas de calidad del agrupamiento\n",
    "def evaluate_clustering_algorithm(algorithm, embeddings, data_authority):\n",
    "    # Ajustar el algoritmo al conjunto de datos\n",
    "    algorithm.fit(embeddings)\n",
    "    \n",
    "    # Obtener las etiquetas de los clústeres y las distancias a los centros\n",
    "    labels = algorithm.labels_ if hasattr(algorithm, 'labels_') else None\n",
    "    if labels is None:\n",
    "        labels = algorithm.predict(embeddings)  # En caso de GMM, usamos 'predict' para obtener las etiquetas\n",
    "    \n",
    "    # 4. Calcular las distancias a los centros de los clústeres (para KMeans)\n",
    "    if isinstance(algorithm, KMeans):\n",
    "        distances_to_centers = np.linalg.norm(embeddings - algorithm.cluster_centers_[labels], axis=1)\n",
    "    else:\n",
    "        distances_to_centers = np.linalg.norm(embeddings - np.mean(embeddings, axis=0), axis=1)  # Aproximación\n",
    "\n",
    "    # 5. Normalizar las distancias a la escala de 1 a 10\n",
    "    scaler = MinMaxScaler(feature_range=(1, 10))\n",
    "    distances_normalized = scaler.fit_transform(distances_to_centers.reshape(-1, 1))\n",
    "\n",
    "    # 6. Crear un DataFrame con los resultados de las intensidades\n",
    "    results = pd.DataFrame({\n",
    "        'Text': data_authority[\"txt\"].tolist(),\n",
    "        'Intensity': distances_normalized.flatten(),\n",
    "        'Cluster': labels\n",
    "    })\n",
    "\n",
    "    # 7. Calcular las métricas de calidad del agrupamiento\n",
    "    cluster_metrics = []\n",
    "    total_samples = len(embeddings)  # Número total de muestras\n",
    "    \n",
    "    for cluster in np.unique(labels):\n",
    "        intensities_for_cluster = results[results['Cluster'] == cluster]['Intensity']\n",
    "        \n",
    "        # Calcular las métricas\n",
    "        avg_intensity = intensities_for_cluster.mean()\n",
    "        std_dev_intensity = intensities_for_cluster.std()\n",
    "        variance_intensity = intensities_for_cluster.var()\n",
    "        kurtosis_intensity = stats.kurtosis(intensities_for_cluster)\n",
    "        min_intensity = intensities_for_cluster.min()\n",
    "        max_intensity = intensities_for_cluster.max()\n",
    "\n",
    "        # Obtener el rango de intensidad\n",
    "        intensity_range = f\"De {min_intensity:.2f} a {max_intensity:.2f}\"\n",
    "\n",
    "        # Calcular cohesión, separación y silueta (usando silhouette_score)\n",
    "        cohesion = np.mean(np.linalg.norm(embeddings[labels == cluster] - np.mean(embeddings[labels == cluster], axis=0), axis=1))\n",
    "        other_clusters_data = embeddings[labels != cluster]\n",
    "        separation = np.mean(np.linalg.norm(embeddings[labels == cluster] - other_clusters_data.mean(axis=0), axis=1))\n",
    "        silhouette_score_value = silhouette_score(embeddings, labels)\n",
    "\n",
    "        # Cálculo del Quality Score (ejemplo basado en un ponderado simple)\n",
    "        quality_score = (cohesion + separation + silhouette_score_value) / 3  # Ajusta la fórmula según lo que desees\n",
    "\n",
    "        # Calcular el número de muestras en el clúster\n",
    "        cluster_size = np.sum(labels == cluster)\n",
    "        \n",
    "        # Añadir las métricas al grupo\n",
    "        cluster_metrics.append({\n",
    "            'Cluster': cluster,\n",
    "            'Average Silhouette Score': silhouette_score_value,\n",
    "            'Cohesion': cohesion,\n",
    "            'Separation': separation,\n",
    "            'Average Intensity': avg_intensity,\n",
    "            'Standard Deviation': std_dev_intensity,\n",
    "            'Variance': variance_intensity,\n",
    "            'Kurtosis': kurtosis_intensity,\n",
    "            'Intensity Range': intensity_range,\n",
    "            'Quality Score': quality_score,\n",
    "            'Cluster Size': f'{cluster_size}/{total_samples}'  # Formato \"10/490\"\n",
    "        })\n",
    "\n",
    "    # Crear el DataFrame final con todas las métricas\n",
    "    metrics_df = pd.DataFrame(cluster_metrics)\n",
    "    \n",
    "    return metrics_df\n",
    "\n",
    "# 8. Evaluar cada algoritmo de agrupamiento\n",
    "all_metrics = {}\n",
    "\n",
    "for name, algorithm in clustering_algorithms.items():\n",
    "    print(f\"\\nEvaluando el algoritmo {name}\")\n",
    "    metrics_df = evaluate_clustering_algorithm(algorithm, embeddings, data_authority)\n",
    "    all_metrics[name] = metrics_df\n",
    "\n",
    "# Mostrar las métricas de todos los algoritmos usando PrettyTable\n",
    "for name, metrics_df in all_metrics.items():\n",
    "    print(f\"\\nMétricas para {name}:\")\n",
    "\n",
    "    # Crear la tabla con PrettyTable\n",
    "    table = PrettyTable()\n",
    "    table.field_names = metrics_df.columns.tolist()\n",
    "\n",
    "    # Agregar filas de datos a la tabla\n",
    "    for row in metrics_df.values:\n",
    "        table.add_row(row)\n",
    "\n",
    "    # Mostrar la tabla en la consola\n",
    "    print(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN, OPTICS\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Función para evaluar un algoritmo de clustering\n",
    "def evaluate_clustering_algorithm(algorithm, embeddings, data_authority):\n",
    "    # Ajustar el algoritmo al conjunto de datos\n",
    "    algorithm.fit(embeddings)\n",
    "    \n",
    "    # Obtener etiquetas de los clústeres\n",
    "    labels = algorithm.labels_ if hasattr(algorithm, 'labels_') else algorithm.predict(embeddings)\n",
    "    \n",
    "    # Calcular distancias a los centros (si aplica)\n",
    "    if isinstance(algorithm, KMeans):\n",
    "        distances_to_centers = np.linalg.norm(embeddings - algorithm.cluster_centers_[labels], axis=1)\n",
    "    elif isinstance(algorithm, GaussianMixture):\n",
    "        distances_to_centers = -algorithm.score_samples(embeddings)\n",
    "    else:\n",
    "        distances_to_centers = np.linalg.norm(embeddings - np.mean(embeddings, axis=0), axis=1)\n",
    "    \n",
    "    # Normalizar las distancias entre 1 y 10\n",
    "    scaler = MinMaxScaler(feature_range=(1, 10))\n",
    "    distances_normalized = scaler.fit_transform(distances_to_centers.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Crear DataFrame de resultados\n",
    "    results = pd.DataFrame({\n",
    "        'Text': data_authority[\"txt\"].tolist(),\n",
    "        'Intensity': distances_normalized,\n",
    "        'Cluster': labels\n",
    "    })\n",
    "\n",
    "    # Calcular métricas por clúster\n",
    "    cluster_metrics = []\n",
    "    total_samples = len(embeddings)\n",
    "    \n",
    "    for cluster in np.unique(labels):\n",
    "        intensities = results[results['Cluster'] == cluster]['Intensity']\n",
    "        avg_intensity = intensities.mean()\n",
    "        std_dev_intensity = intensities.std()\n",
    "        intensity_range = f\"{intensities.min():.2f} - {intensities.max():.2f}\"\n",
    "        cohesion = (\n",
    "            np.mean(\n",
    "                np.linalg.norm(embeddings[labels == cluster] - np.mean(embeddings[labels == cluster], axis=0), axis=1)\n",
    "            ) if cluster != -1 else np.nan  # Ignorar cohesión para ruido\n",
    "        )\n",
    "        silhouette_value = silhouette_score(embeddings, labels) if len(np.unique(labels)) > 1 else np.nan\n",
    "        cluster_size = np.sum(labels == cluster)\n",
    "        \n",
    "        cluster_metrics.append({\n",
    "            'Cluster': cluster,\n",
    "            'Average Intensity': avg_intensity,\n",
    "            'Intensity Range': intensity_range,\n",
    "            'Cohesion': cohesion,\n",
    "            'Silhouette Score': silhouette_value,\n",
    "            'Cluster Size': f'{cluster_size}/{total_samples}'\n",
    "        })\n",
    "    \n",
    "    # Crear DataFrame con métricas\n",
    "    metrics_df = pd.DataFrame(cluster_metrics)\n",
    "    \n",
    "    # Añadir columna \"Average Intensity Normalized\" al DataFrame original\n",
    "    scaler_avg = MinMaxScaler(feature_range=(1, 10))\n",
    "    metrics_df['Average Intensity Normalized'] = scaler_avg.fit_transform(metrics_df[['Average Intensity']])\n",
    "    \n",
    "    return results, metrics_df, labels\n",
    "\n",
    "# Visualización de los clústeres\n",
    "def plot_clusters(embeddings, labels, title=\"Visualización de Clústeres\"):\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced_data = pca.fit_transform(embeddings)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    unique_clusters = np.unique(labels)\n",
    "    for cluster in unique_clusters:\n",
    "        cluster_data = reduced_data[labels == cluster]\n",
    "        plt.scatter(cluster_data[:, 0], cluster_data[:, 1], label=f\"Cluster {cluster}\" if cluster != -1 else \"Ruido\")\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Componente Principal 1\")\n",
    "    plt.ylabel(\"Componente Principal 2\")\n",
    "    plt.legend(title=\"Clústeres\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Pipeline para evaluar múltiples algoritmos\n",
    "def clustering_pipeline(embeddings, data_authority):\n",
    "    algorithms = {\n",
    "        'KMeans': KMeans(n_clusters=10, random_state=42),\n",
    "        'DBSCAN': DBSCAN(eps=0.5, min_samples=5),\n",
    "        'OPTICS': OPTICS(min_samples=5, cluster_method='xi'),\n",
    "        'GMM': GaussianMixture(n_components=10, random_state=42)\n",
    "    }\n",
    "    \n",
    "    for name, algorithm in algorithms.items():\n",
    "        print(f\"\\nEvaluando {name}\")\n",
    "        results, metrics_df, labels = evaluate_clustering_algorithm(algorithm, embeddings, data_authority)\n",
    "        \n",
    "        # Mostrar métricas\n",
    "        print(metrics_df)\n",
    "        \n",
    "        # Visualizar clústeres\n",
    "        plot_clusters(embeddings, labels, title=f\"Visualización de Clústeres - {name}\")\n",
    "\n",
    "# Ejemplo de uso\n",
    "# Supongamos que 'embeddings' es tu matriz de datos y 'data_authority' contiene los textos originales\n",
    "# embeddings = ... # Carga tus datos aquí\n",
    "# data_authority = ... # Carga tus textos aquí\n",
    "clustering_pipeline(embeddings, data_authority)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
